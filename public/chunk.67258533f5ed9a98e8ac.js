/*! For license information please see chunk.67258533f5ed9a98e8ac.js.LICENSE.txt */
"use strict";(self.webpackChunkai_workout_assistant=self.webpackChunkai_workout_assistant||[]).push([[858],{28819:(t,e,s)=>{s.d(e,{GD:()=>$,Gc:()=>y,aI:()=>v});var n=s(88478),i=s(39840),a=s(2931);class o extends n.m7h.Serializable{getConfig(){return{}}}class r extends o{apply(t,e=1){return i.py(t,e)}}r.className="elu",n.m7h.registerClass(r);class h extends o{apply(t){return n.U8D(t)}}h.className="selu",n.m7h.registerClass(h);class l extends o{apply(t){return n.UYe(t)}}l.className="relu",n.m7h.registerClass(l);class u extends o{apply(t){return(0,n.lub)((()=>n.LTh(6,n.UYe(t))))}}u.className="relu6",n.m7h.registerClass(u);class c extends o{apply(t){return t}}c.className="linear",n.m7h.registerClass(c);class p extends o{apply(t){return n.XD2(t)}}p.className="sigmoid",n.m7h.registerClass(p);class d extends o{apply(t){return i.HX(t)}}d.className="hardSigmoid",n.m7h.registerClass(d);class f extends o{apply(t){return n.Wvh(t)}}f.className="softplus",n.m7h.registerClass(f);class g extends o{apply(t){return i.O(t)}}g.className="softsign",n.m7h.registerClass(g);class m extends o{apply(t){return n.AEp(t)}}m.className="tanh",n.m7h.registerClass(m);class y extends o{apply(t,e=-1){return n.XAC(t,e)}}y.className="softmax",n.m7h.registerClass(y);class b extends o{apply(t,e=-1){return n.CmS(t,e)}}b.className="logSoftmax",n.m7h.registerClass(b);class w extends o{apply(t,e=1){return(0,n.lub)((()=>n.dC7(n.XD2(n.dC7(t,e)),t)))}}w.className="swish",n.m7h.registerClass(w);class x extends o{apply(t){return(0,n.lub)((()=>n.dC7(t,n.AEp(n.Wvh(t)))))}}function $(t){return t.getClassName()}function N(t,e={}){return(0,a.tU)(t,n.m7h.SerializationMap.getMap().classNameMap,e,"activation")}function v(t){if(null==t){return N({className:"linear",config:{}})}if("string"==typeof t){const e={};return e.className=t,e.config={},N(e)}return t instanceof o?t:N(t)}x.className="mish",n.m7h.registerClass(x)},12012:(t,e,s)=>{s.d(e,{Ho:()=>a,rf:()=>o});var n=s(88478);let i;function a(){return null==i&&(i=(0,n.y3$)().epsilon()),i}function o(){return"channelsLast"}},79608:(t,e,s)=>{s.d(e,{L:()=>i,s:()=>o});let n=0;function i(){return n++}const a={};function o(t=""){return t in a||(a[t]=0),a[t]+=1,t+a[t].toString()}},39840:(t,e,s)=>{s.d(e,{AK:()=>x,GZ:()=>y,Gg:()=>b,HX:()=>S,Iq:()=>$,KC:()=>E,O:()=>T,Uz:()=>p,a2:()=>k,c9:()=>d,dt:()=>l,h6:()=>N,mV:()=>m,nG:()=>w,pj:()=>h,py:()=>L,rv:()=>I,rx:()=>u,uI:()=>g,xH:()=>c});var n=s(88478),i=s(48090),a=s(40588),o=s(96040),r=s(12012);function h(t,e){return n.pju(t,e)}function l(t,e=-1){const s=t.shape.slice();return e<0&&(e=s.length+e+1),s.splice(e,0,1),n.XLQ(t,s)}function u(t,e){return(0,n.lub)((()=>{if(2!==t.shape.length)throw new a.nu(`repeat() expects a rank-2 tensor, but received a rank-${t.shape.length} tensor.`);return b(l(t,1),[1,e,1])}))}function c(t){const e=[o.NS(t.shape)];return n.XLQ(t,e)}function p(t){if(t.rank<=1)throw new a.nu(`batchFlatten requires a minimum rank of 2. Got rank: ${t.rank}.`);const e=[t.shape[0],o.NS(t.shape,1)];return n.XLQ(t,e)}function d(t,e,s){return(0,n.lub)((()=>{switch(t.rank){case 1:return n.jZU(t,e,s);case 2:return n.SmN(t,[e,0],[s,t.shape[1]]);case 3:return n.CnO(t,[e,0,0],[s,t.shape[1],t.shape[2]]);case 4:return n.p0P(t,[e,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3]]);case 5:return n.tPi(t,[e,0,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3],t.shape[4]]);case 6:return n.tPi(t,[e,0,0,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3],t.shape[4],t.shape[5]]);default:throw new a.nu(`sliceAlongFirstAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function f(t,e,s){return(0,n.lub)((()=>{switch(t.rank){case 1:return n.jZU(t,e,s);case 2:return n.SmN(t,[0,e],[t.shape[0],s]);case 3:return n.CnO(t,[0,0,e],[t.shape[0],t.shape[1],s]);case 4:return n.p0P(t,[0,0,0,e],[t.shape[0],t.shape[1],t.shape[2],s]);default:throw new a.nu(`sliceAlongLastAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function g(t,e,s,i){return(0,n.lub)((()=>{switch(t.rank){case 1:return n.jZU(t,e,s);case 2:switch(i){case 1:return d(t,e,s);case 2:return f(t,e,s);default:throw new a.nu(`The axis is not within the rank of the tensor ${i}`)}case 3:switch(i){case 1:return d(t,e,s);case 2:return n.CnO(t,[0,e,0],[t.shape[0],s,t.shape[2]]);case 3:return f(t,e,s);default:throw new a.nu(`The axis is not within the rank of the tensor ${i}`)}case 4:switch(i){case 1:return d(t,e,s);case 2:return n.p0P(t,[0,e,0,0],[t.shape[0],s,t.shape[2],t.shape[3]]);case 3:return n.p0P(t,[0,0,e,0],[t.shape[0],t.shape[1],s,t.shape[3]]);case 4:return f(t,e,s);default:throw new a.nu(`The axis is not within the rank of the tensor ${i}`)}default:throw new a.nu(`sliceAlongLastAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function m(t,e=-1){let s;return e<0&&(s=t[0].rank,e=0!==s?s:0),e===t[0].rank&&(e=-1),n.zoF(t,e)}function y(t,e){switch(t.rank){case 1:return n.gME([t,e]);case 2:return n.Izb([t,e],0);case 3:return n.MNy([t,e],0);case 4:return n.ZaL([t,e],0);default:throw new a.nu(`concatAlongFirstAxis() received an unsupported tensor rank: ${t.rank}`)}}function b(t,e){if(Array.isArray(e)||(e=[e]),t.rank!==e.length)throw new a.nu(`The length of input n (${e.length}) does not match the number of dimensions in input x (${t.rank})`);return n.Gg6(t,e)}function w(t,e=0,s=1,i,a){return n.nGf(t,e,s,i,a)}function x(t,e,s,i){if(t.rank<2||e.rank<2)throw new a.nj(`dot requires both inputs to be rank >= 2 but got x shape = ${t.shape} and y shape = ${e.shape}`);if(e.rank>=3&&t.shape.slice(-1)[0]!==e.shape.slice(-2)[0])throw new a.nj(`If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = ${t.shape} and  y shape = ${e.shape}`);if(2===t.rank&&2===e.rank){const a=!1,o=!1;return n.imm.matMul({a:t,b:e,transposeA:a,transposeB:o,bias:i?v(t.rank,i,(0,r.rf)()):null,activation:s})}{const a=t.shape.slice(),o=a.pop();t=n.XLQ(t,[-1,o]);const h=e.shape.slice(),l=h.pop(),u=h.pop(),c=[...h,l],p=Array.from({length:e.rank},((t,s)=>0===s?e.rank-2:s<=e.rank-2?s-1:s));e=n.XLQ(n.p4s(e,p),[u,-1]);const d=[...a,...c],f=!1,g=!1;return n.XLQ(n.imm.matMul({a:t,b:e,transposeA:f,transposeB:g,bias:i?v(t.rank,i,(0,r.rf)()):null,activation:s}),d)}}function $(t,e,s){return(0,n.lub)((()=>(e=Array.isArray(e)?(0,n.RRF)(e,"int32"):n.pju(e,"int32"),n.Iqj(t,e,s))))}function N(t){return n.dC7(t,t)}function v(t,e,s){const i=e.shape;if(1!==e.rank&&e.rank!==t)throw new a.nu(`Unexpected bias dimensions: ${e.rank}; expected it to be 1 or ${t}`);if(5===t){if("channelsFirst"===s)return 1===i.length?n.XLQ(e,[1,i[0],1,1,1]):n.XLQ(e,[1,i[3],i[0],i[1],i[2]]);if("channelsLast"===s)return 1===i.length?n.XLQ(e,[1,1,1,1,i[0]]):n.XLQ(e,[1].concat(i))}else if(4===t){if("channelsFirst"===s)return 1===i.length?n.XLQ(e,[1,i[0],1,1]):n.XLQ(e,[1,i[2],i[0],i[1]]);if("channelsLast"===s)return 1===i.length?n.XLQ(e,[1,1,1,i[0]]):n.XLQ(e,[1].concat(i))}else if(3===t){if("channelsFirst"===s)return 1===i.length?n.XLQ(e,[1,i[0],1]):n.XLQ(e,[1,i[1],i[0]]);if("channelsLast"===s)return 1===i.length?n.XLQ(e,[1,1,i[0]]):n.XLQ(e,[1].concat(i))}else if(t<3)return e;throw new a.nu(`Unsupported input rank by biasAdd: ${e.rank}`)}function k(t,e,s){return(0,n.lub)((()=>(null==s&&(s=(0,r.rf)()),(0,i.cj)(s),n.IHx(t,v(t.rank,e,s)))))}function L(t,e=1){if(1!==e)throw new a.nj(`Support for alpha values other than 1 (${e}) is not implemented yet.`);return n.pyx(t)}function T(t){return(0,n.lub)((()=>n.hiC(t,n.IHx(n.WnP(t),1))))}function I(t,e,s,i){return(0,n.lub)((()=>n.rvX(t,e,s,i)))}function S(t){return(0,n.lub)((()=>{const e=n.IHx(.5,n.dC7(.2,t));return n.iUl(e,0,1)}))}function E(t,e,s=!1){return s?t():e()}},28891:(t,e,s)=>{s.d(e,{CZ:()=>d,ex:()=>h,m$:()=>g});var n,i=s(88478),a=s(40588),o=s(73146),r=s(2931);!function(t){t[t.SILENT=0]="SILENT",t[t.VERBOSE=1]="VERBOSE"}(n||(n={}));class h{constructor(){this.validationData=null}setParams(t){this.params=t}async onEpochBegin(t,e){}async onEpochEnd(t,e){}async onBatchBegin(t,e){}async onBatchEnd(t,e){}async onTrainBegin(t){}async onTrainEnd(t){}setModel(t){}}class l{constructor(t,e=10){null==t&&(t=[]),this.callbacks=t,this.queueLength=e}append(t){this.callbacks.push(t)}setParams(t){for(const e of this.callbacks)e.setParams(t)}setModel(t){for(const e of this.callbacks)e.setModel(t)}async onEpochBegin(t,e){null==e&&(e={});for(const s of this.callbacks)await s.onEpochBegin(t,e)}async onEpochEnd(t,e){null==e&&(e={});for(const s of this.callbacks)await s.onEpochEnd(t,e)}async onBatchBegin(t,e){null==e&&(e={});for(const s of this.callbacks)await s.onBatchBegin(t,e)}async onBatchEnd(t,e){null==e&&(e={});for(const s of this.callbacks)await s.onBatchEnd(t,e)}async onTrainBegin(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainBegin(t)}async onTrainEnd(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainEnd(t)}}class u extends h{constructor(){super()}async onEpochBegin(t){this.seen=0,this.totals={}}async onBatchEnd(t,e){null==e&&(e={});const s=null==e.size?0:e.size;this.seen+=s;for(const t in e){const n=e[t];if("number"==typeof n)this.totals.hasOwnProperty(t)||(this.totals[t]=0),this.totals[t]=this.totals[t]+n*s;else{let e;t in this.totals?e=this.totals[t]:this.totals[t]=0;const a=(0,i.lub)((()=>(0,i.IHx)(this.totals[t],(0,i.dC7)(n,s))));this.totals[t]=a,null!=e&&e.dispose()}}}async onEpochEnd(t,e){if(null!=e)for(const t of this.params.metrics)null!=this.totals[t]&&("number"==typeof this.totals[t]?e[t]=this.totals[t]/this.seen:(0,i.lub)((()=>{const s=(0,i.dC7)((0,i.hiC)(1,this.seen),this.totals[t]);e[t]=s,this.totals[t].dispose(),(0,i.CnY)(e[t])})))}}class c extends h{async onTrainBegin(t){this.epoch=[],this.history={}}async onEpochEnd(t,e){null==e&&(e={}),this.epoch.push(t);for(const t in e)null==this.history[t]&&(this.history[t]=[]),this.history[t].push(e[t])}async syncData(){const t=[],e=[],s=[];for(const n in this.history){const i=this.history[n];for(let a=0;a<i.length;++a)if("number"!=typeof i[a]){const o=i[a];t.push(o.data()),e.push(n),s.push(a)}}const n=await Promise.all(t);for(let t=0;t<n.length;++t)this.history[e[t]][s[t]].dispose(),this.history[e[t]][s[t]]=n[t][0]}}class p extends h{constructor(t,e){if(super(),this.currentEpoch=0,this.nowFunc=t.nowFunc,this.nextFrameFunc=t.nextFrameFunc||i.glt,this.yieldEvery=e||"auto","auto"===this.yieldEvery&&(this.yieldEvery=125),"never"===this.yieldEvery&&null!=t.onYield)throw new Error("yieldEvery is `never` but you provided an `onYield` callback. Either change `yieldEvery` or remove the callback");i.D5U.isNumber(this.yieldEvery)&&(this.maybeWait=r.Ds(this.maybeWait.bind(this),this.yieldEvery,this.nowFunc)),this.trainBegin=t.onTrainBegin,this.trainEnd=t.onTrainEnd,this.epochBegin=t.onEpochBegin,this.epochEnd=t.onEpochEnd,this.batchBegin=t.onBatchBegin,this.batchEnd=t.onBatchEnd,this.yield=t.onYield}async maybeWait(t,e,s){const n=[];null!=this.yield&&(await(0,o.Z)(s),n.push(this.yield(t,e,s))),n.push(this.nextFrameFunc()),await Promise.all(n)}async onEpochBegin(t,e){this.currentEpoch=t,null!=this.epochBegin&&(await(0,o.Z)(e),await this.epochBegin(t,e))}async onEpochEnd(t,e){const s=[];null!=this.epochEnd&&(await(0,o.Z)(e),s.push(this.epochEnd(t,e))),"epoch"===this.yieldEvery&&s.push(this.nextFrameFunc()),await Promise.all(s)}async onBatchBegin(t,e){null!=this.batchBegin&&(await(0,o.Z)(e),await this.batchBegin(t,e))}async onBatchEnd(t,e){const s=[];null!=this.batchEnd&&(await(0,o.Z)(e),s.push(this.batchEnd(t,e))),"batch"===this.yieldEvery?s.push(this.nextFrameFunc()):i.D5U.isNumber(this.yieldEvery)&&s.push(this.maybeWait(this.currentEpoch,t,e)),await Promise.all(s)}async onTrainBegin(t){null!=this.trainBegin&&(await(0,o.Z)(t),await this.trainBegin(t))}async onTrainEnd(t){null!=this.trainEnd&&(await(0,o.Z)(t),await this.trainEnd(t))}}function d(t,e){return null==t&&(t={}),t instanceof h?[t]:Array.isArray(t)&&t[0]instanceof h?t:r.zZ(t).map((t=>new p(t,e)))}class f{constructor(){}static registerCallbackConstructor(t,e){i.D5U.assert(t>=0&&Number.isInteger(t),(()=>`Verbosity level is expected to be an integer >= 0, but got ${t}`)),f.checkForDuplicate(e),null==f.constructors[t]&&(f.constructors[t]=[]),f.constructors[t].push(e)}static checkForDuplicate(t){for(const e in f.constructors)f.constructors[+e].forEach((e=>{if(e===t)throw new a.nu("Duplicate callback constructor.")}))}static clear(){f.constructors={}}static createCallbacks(t){const e=[];for(const s in f.constructors){const n=+s;t>=n&&e.push(...f.constructors[n])}return e.map((t=>new t))}}function g(t,e,s,n,i,a,o,r,h){const p=new c,d=[new u,...f.createCallbacks(e)];null!=t&&d.push(...t),d.push(p);const g=new l(d);return g.setParams({epochs:s,initialEpoch:n,samples:i,steps:a,batchSize:o,verbose:e,doValidation:r,metrics:h}),{callbackList:g,history:p}}f.constructors={}},66084:(t,e,s)=>{var n=s(28891);s(5540),s(40588),s(73146);class i extends n.ex{constructor(){super(...arguments),this.model=null}setModel(t){if(!(t instanceof _engine_training__WEBPACK_IMPORTED_MODULE_1__.QV))throw new Error("model must be a LayersModel, not some other Container");this.model=t}}},48090:(t,e,s)=>{s.d(e,{Lp:()=>l,MU:()=>p,cj:()=>o,f4:()=>c,w8:()=>d,wU:()=>r,zb:()=>h});var n=s(44685),i=s(2931);const a=new Map;function o(t){(0,i.xn)(n.PS,"DataFormat",t)}function r(t){(0,i.xn)(n.Mz,"InterpolationFormat",t)}function h(t){(0,i.xn)(n.zx,"PaddingMode",t)}function l(t){(0,i.xn)(n.MK,"PoolMode",t)}const u=[];function c(t,e){u.push(t);try{const t=e();return u.pop(),t}catch(t){throw u.pop(),t}}function p(t){if(!g(t))throw new Error("Not a valid tensor name: '"+t+"'");return(0===u.length?"":u.join("/")+"/")+t}function d(t){if(!g(t))throw new Error("Not a valid tensor name: '"+t+"'");a.has(t)||a.set(t,0);const e=a.get(t);if(a.set(t,a.get(t)+1),e>0){const s=`${t}_${e}`;return a.set(s,1),s}return t}const f=new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\._\/]*$/);function g(t){return!!t.match(f)}},64079:(t,e,s)=>{s.d(e,{Ad:()=>g,xF:()=>d});var n=s(88478),i=s(12012),a=s(2931);function o(t,e){return(0,n.lub)((()=>n._b3(n.Smz(n.dC7(t,t),e,!0))))}class r extends n.m7h.Serializable{getConfig(){return{}}}class h extends r{constructor(t){super(),this.defaultMaxValue=2,this.defaultAxis=0,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return(0,n.lub)((()=>{const e=o(t,this.axis),s=n.iUl(e,0,this.maxValue);return n.dC7(t,n.hiC(s,n.IHx((0,i.Ho)(),e)))}))}getConfig(){return{maxValue:this.maxValue,axis:this.axis}}}h.className="MaxNorm",n.m7h.registerClass(h);class l extends r{constructor(t){super(),this.defaultAxis=0,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return(0,n.lub)((()=>n.hiC(t,n.IHx((0,i.Ho)(),o(t,this.axis)))))}getConfig(){return{axis:this.axis}}}l.className="UnitNorm",n.m7h.registerClass(l);class u extends r{apply(t){return n.UYe(t)}}u.className="NonNeg",n.m7h.registerClass(u);class c extends r{constructor(t){super(),this.defaultMinValue=0,this.defaultMaxValue=1,this.defaultRate=1,this.defaultAxis=0,this.minValue=null!=t.minValue?t.minValue:this.defaultMinValue,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.rate=null!=t.rate?t.rate:this.defaultRate,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return(0,n.lub)((()=>{const e=o(t,this.axis),s=n.IHx(n.dC7(this.rate,n.iUl(e,this.minValue,this.maxValue)),n.dC7(1-this.rate,e));return n.dC7(t,n.hiC(s,n.IHx((0,i.Ho)(),e)))}))}getConfig(){return{minValue:this.minValue,maxValue:this.maxValue,rate:this.rate,axis:this.axis}}}c.className="MinMaxNorm",n.m7h.registerClass(c);const p={maxNorm:"MaxNorm",minMaxNorm:"MinMaxNorm",nonNeg:"NonNeg",unitNorm:"UnitNorm"};function d(t){return(0,a.Kj)(t)}function f(t,e={}){return(0,a.tU)(t,n.m7h.SerializationMap.getMap().classNameMap,e,"constraint")}function g(t){return null==t?null:"string"==typeof t?f({className:t in p?p[t]:t,config:{}}):t instanceof r?t:f(t)}},64843:(t,e,s)=>{s.d(e,{ht:()=>d,kS:()=>p,l2:()=>l});var n=s(88478),i=s(40588),a=s(6247),o=s(2931),r=s(34396),h=s(20163);class l{constructor(t){if(this.id2Value={},this.id2Mask={},this.name2Id={},t instanceof l)for(const e in t.id2Value)this.id2Value[e]=t.id2Value[e],e in t.id2Mask&&(this.id2Mask[e]=t.id2Mask[e]);else{if(null==t)return;for(const e of t)this.add(e.key,e.value)}}add(t,e,s){if(null!=this.id2Value[t.id])throw new i.nu(`Duplicate key: name=${t.name}, id=${t.id}`);return this.id2Value[t.id]=function(t,e){if(null==t.dtype||t.dtype===e.dtype)return e;try{return(0,n.pju)(e,t.dtype)}catch(s){throw new i.nu(`The dtype of the feed (${e.dtype}) can not be cast to the dtype of the key '${t.name}' (${t.dtype}).`)}}(t,e),this.name2Id[t.name]=t.id,null!=s&&(this.id2Mask[t.id]=s),this}addFeed(t){this.add(t.key,t.value)}hasKey(t){return null!=this.id2Value[t.id]}names(){return Object.keys(this.name2Id)}getValue(t){if(t instanceof h.Iy){if(null==this.id2Value[t.id])throw new i.nu(`Nonexistent key: ${t.name}`);return this.id2Value[t.id]}{const e=this.name2Id[t];if(null==e)throw new i.nu(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Value[e]}}getMask(t){if(t instanceof h.Iy){if(null==this.id2Value[t.id])throw new i.nu(`Nonexistent key: ${t.name}`);return this.id2Mask[t.id]}{const e=this.name2Id[t];if(null==e)throw new i.nu(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Mask[e]}}disposeMasks(){null!=this.id2Mask&&(0,n.B90)(this.id2Mask)}}const u=new a.s,c=new a.s;function p(t){null!=u&&u.setMaxEntries(t),null!=c&&c.setMaxEntries(t)}function d(t,e,s,i){const a=null!=s&&s.training,h=Array.isArray(t),p=h?t:[t],d=p.map((t=>t.name)),y=[],b=e.names();for(const t of d)-1!==b.indexOf(t)?y.push(e.getValue(t)):y.push(null);null!=i&&(i.maxNumTensors=-1/0,i.minNumTensors=1/0);const w=d.join(",")+"|"+e.names().sort().join(",");let x,$=u.get(w);if(null==$){const t=function(t,e){n.D5U.assert(null!=t&&t.length>0,(()=>"Expected at least one fetch, got none"));let s=[],i={};if(1===t.length){const n=g(t[0],e);s=n.sorted,i=n.recipientMap}else{const n=new Set;for(const a of t){const{sorted:t,recipientMap:o}=g(a,e);for(const e of t)n.has(e.name)||(s.push(e),n.add(e.name));for(const t in o)null==i[t]&&(i[t]=new Set),o[t].forEach((e=>i[t].add(e)))}}return{sorted:s,recipientCounts:f(i)}}(p,e);$=t.sorted,x=t.recipientCounts,u.put(w,$),c.put(w,x)}x={},a||Object.assign(x,c.get(w));const N=new l(e);for(let t=0;t<$.length;++t){if(null!=i){const t=(0,n.sq6)().numTensors;t>i.maxNumTensors&&(i.maxNumTensors=t),t<i.minNumTensors&&(i.minNumTensors=t)}const h=$[t],l=h.sourceLayer;if(l instanceof r.l)continue;const u=[],c=[],p=[];let f=!1;for(const t of h.inputs){const s=N.getValue(t),n=N.getMask(t);u.push(s),c.push(n),null!=n&&(f=!0),a||(x[t.name]--,0!==x[t.name]||e.hasKey(t)||-1!==d.indexOf(t.name)||s.isDisposed||!0===t.sourceLayer.stateful||p.push(s))}f&&((s=s||{}).mask=c[0]);const g=(0,o.zZ)(l.apply(u,s));let b=null;l.supportsMasking&&(b=l.computeMask(u,c));const w=m(h),v=Array.isArray(w)?w:[w];for(let t=0;t<v.length;++t){N.hasKey(v[t])||N.add(v[t],g[t],Array.isArray(b)?b[0]:b);const e=d.indexOf(v[t].name);-1!==e&&(y[e]=g[t])}a||(0,n.B90)(p)}return N.disposeMasks(),h?y:y[0]}function f(t){const e={};for(const s in t)e[s]=t[s].size;return e}function g(t,e){const s=new Set,n=[],i={};for(const t of e.names())s.add(t);const a=[],o=[];for(a.push(t);a.length>0;){const t=a[a.length-1];if(s.has(t.name)){a.pop();continue}const e=o[o.length-1]===a.length-1;if(0===t.inputs.length||e)a.pop(),n.push(t),s.add(t.name),e&&o.pop();else{o.push(a.length-1);for(const e of t.inputs)null==i[e.name]&&(i[e.name]=new Set),i[e.name].add(t.name),s.has(e.name)||a.push(e)}}return{sorted:n,recipientMap:i}}function m(t){let e;if(1===t.sourceLayer.inboundNodes.length)e=t.sourceLayer.output;else{let s=null;for(let e=0;e<t.sourceLayer.inboundNodes.length;++e)for(const n of t.sourceLayer.inboundNodes[e].outputTensors)if(n.id===t.id){s=e;break}e=t.sourceLayer.getOutputAt(s)}return e}},34396:(t,e,s)=>{s.d(e,{I:()=>h,l:()=>r});var n=s(88478),i=s(79608),a=s(40588),o=s(20163);class r extends o.mh{constructor(t){if(super({dtype:t.dtype,name:null!=t.name?t.name:(0,i.s)("input").toString()}),null==t.batchSize&&(t.batchSize=null),null==t.sparse&&(t.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=t.sparse,null!=t.inputShape&&null!=t.batchInputShape)throw new a.nu("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let e=t.batchInputShape;if(null==e){if(null==t.inputShape)throw new a.nu("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");e=[t.batchSize].concat(t.inputShape)}else if(null!=t.batchSize)throw new a.nu("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const s=t.dtype||"float32";this.batchInputShape=e,this.dtype=s,this.inputSpec=[{shape:e}];const n=new o.Iy(this.dtype,this.batchInputShape,this,[],{},this.name);n.nodeIndex=0,n.tensorIndex=0,new o.NB({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[n],outputTensors:[n],inputMasks:[null],outputMasks:[null],inputShapes:[e],outputShapes:[e]})}apply(t,e){throw new a.nu(`Cannot pass any input to an InputLayer's apply() method. InputLayer name: ${this.name}`)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}function h(t){if(null==t.batchShape&&null==t.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=t.batchShape&&null!=t.shape)throw new a.nu("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let e=t.batchShape;null!=t.shape&&null==e&&(e=[null].concat(t.shape));let s=t.dtype;return null==s&&(s="float32"),new r({batchInputShape:e,name:t.name,dtype:s,sparse:t.sparse}).inboundNodes[0].outputTensors[0]}r.className="InputLayer",n.m7h.registerClass(r)},20163:(t,e,s)=>{s.d(e,{Iy:()=>d,NB:()=>g,Zg:()=>p,hA:()=>b,mh:()=>y});var n=s(88478),i=s(79608),a=s(48090),o=s(40588),r=s(85654),h=s(2931),l=s(87538),u=s(23013),c=s(41653);class p{constructor(t){this.dtype=t.dtype,this.shape=t.shape,null!=t.shape?this.ndim=t.shape.length:this.ndim=t.ndim,this.maxNDim=t.maxNDim,this.minNDim=t.minNDim,this.axes=t.axes||{}}}class d{constructor(t,e,s,n,o,r,h){this.dtype=t,this.shape=e,this.sourceLayer=s,this.inputs=n,this.callArgs=o,this.outputTensorIndex=h,this.id=(0,i.L)(),null!=r&&(this.originalName=(0,a.MU)(r),this.name=(0,a.w8)(this.originalName)),this.rank=e.length}}let f=0;class g{constructor(t,e){this.callArgs=e,this.id=f++,this.outboundLayer=t.outboundLayer,this.inboundLayers=t.inboundLayers,this.nodeIndices=t.nodeIndices,this.tensorIndices=t.tensorIndices,this.inputTensors=t.inputTensors,this.outputTensors=t.outputTensors,this.inputMasks=t.inputMasks,this.outputMasks=t.outputMasks,this.inputShapes=t.inputShapes,this.outputShapes=t.outputShapes;for(const e of t.inboundLayers)null!=e&&e.outboundNodes.push(this);t.outboundLayer.inboundNodes.push(this)}getConfig(){const t=[];for(const e of this.inboundLayers)null!=e?t.push(e.name):t.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:t,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let m=0;class y extends n.m7h.Serializable{constructor(t={}){super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=m++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let e=t.name;if(!e){const t=this.getClassName();e=h.D1(t)+"_"+(0,i.s)(t)}if(this.name=e,this.trainable_=null==t.trainable||t.trainable,null!=t.inputShape||null!=t.batchInputShape){let e;if(null!=t.batchInputShape)e=t.batchInputShape;else if(null!=t.inputShape){let s=null;null!=t.batchSize&&(s=t.batchSize),e=[s].concat(t.inputShape)}this.batchInputShape=e;let s=t.dtype;null==s&&(s=t.inputDType),null==s&&(s="float32"),this.dtype=s}null!=t.weights?this.initialWeights=t.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(t,e){return t.name+"_ib-"+e.toString()}getNodeAtIndex(t,e){if(0===this.inboundNodes.length)throw new o.LH(`The layer has never been called and thus has no defined ${e}.`);if(this.inboundNodes.length<=t)throw new o.nu(`Asked to get ${e} at node ${t}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);return this.inboundNodes[t]}getInputAt(t){return h.Bq(this.getNodeAtIndex(t,"input").inputTensors)}getOutputAt(t){return h.Bq(this.getNodeAtIndex(t,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new o.j1(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use \`getInputAt(nodeIndex)\` instead.`);if(0===this.inboundNodes.length)throw new o.j1(`Layer ${this.name} is not connected, no input to return.`);return h.Bq(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new o.j1(`Layer ${this.name} has no inbound nodes.`);if(this.inboundNodes.length>1)throw new o.j1(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use \`getOutputAt(nodeIndex)\` instead.`);return h.Bq(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map((t=>t()))}get updates(){return this._updates}get built(){return this._built}set built(t){this._built=t}get trainable(){return this.trainable_}set trainable(t){this._trainableWeights.forEach((e=>e.trainable=t)),this.trainable_=t}get trainableWeights(){return this.trainable_?this._trainableWeights.filter((t=>t.trainable)):[]}set trainableWeights(t){this._trainableWeights=t}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter((t=>!t.trainable)).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(t){this._nonTrainableWeights=t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(t){if(t=h.zZ(t),null==this.inputSpec||0===this.inputSpec.length)return;const e=h.zZ(this.inputSpec);if(t.length!==e.length)throw new o.nu(`Layer ${this.name} expects ${e.length} inputs, but it received ${t.length} input tensors. Input received: ${t}`);for(let s=0;s<t.length;s++){const n=t[s],i=e[s];if(null==i)continue;const a=n.rank;if(null!=i.ndim&&a!==i.ndim)throw new o.nu(`Input ${s} is incompatible with layer ${this.name}: expected ndim=${i.ndim}, found ndim=${a}`);if(null!=i.maxNDim&&a>i.maxNDim)throw new o.nu(`Input ${s} is incompatible with layer ${this.name}: expected max_ndim=${i.maxNDim}, found ndim=${a}`);if(null!=i.minNDim&&a<i.minNDim)throw new o.nu(`Input ${s} is incompatible with layer ${this.name}: expected min_ndim=${i.minNDim}, found ndim=${a}.`);if(null!=i.dtype&&n.dtype!==i.dtype)throw new o.nu(`Input ${s} is incompatible with layer ${this.name} : expected dtype=${i.dtype}, found dtype=${n.dtype}.`);if(i.axes){const t=n.shape;for(const e in i.axes){const n=Number(e),a=i.axes[e],r=n>=0?t[n]:t[t.length+n];if(null!=a&&-1===[a,null].indexOf(r))throw new o.nu(`Input ${s} is incompatible with layer ${this.name}: expected axis ${n} of input shape to have value ${a} but got shape ${t}.`)}}if(null!=i.shape)for(let t=0;t<i.shape.length;++t){const e=i.shape[t],a=n.shape[t];if(null!=e&&null!=a&&e!==a)throw new o.nu(`Input ${s} is incompatible with layer ${this.name}: expected shape=${i.shape}, found shape=${n.shape}.`)}}}call(t,e){return t}invokeCallHook(t,e){null!=this._callHook&&this._callHook(t,e)}setCallHook(t){this._callHook=t}clearCallHook(){this._callHook=null}apply(t,e){e=e||{},this.assertNotDisposed();const s=h.zZ(t);let n=!0;for(const t of s)if(!(t instanceof d)){n=!1;break}let i=!0;for(const t of s)if(t instanceof d){i=!1;break}if(n===i)throw new o.nu("Arguments to apply() must be all SymbolicTensors or all Tensors");return(0,a.f4)(this.name,(()=>{if(!this.built){this.assertInputCompatibility(t);const e=[];for(const s of h.zZ(t))e.push(s.shape);this.build(h.Bq(e)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&i&&(this._refCount=1)}if(this.assertInputCompatibility(t),i){let n=this.call(t,e);const i=h.zZ(n),a=[];for(let t of i)-1!==s.indexOf(t)&&(t=t.clone()),a.push(t);if(n=h.Bq(a),null!=this.activityRegularizer)throw new o.nj("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return n}{const s=function(t){t=h.zZ(t);const e=[];for(const s of t)e.push(s.shape);return h.Bq(e)}(t),n=this.computeOutputShape(s);let i;const a="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(t)?s[0]:s),i=null!=n&&n.length>0&&Array.isArray(n[0])?n.map(((s,n)=>new d(a,s,this,h.zZ(t),e,this.name,n))):new d(a,n,this,h.zZ(t),e,this.name),this.addInboundNode(t,i,null,null,s,n,e),this._refCount++,null!=this.activityRegularizer)throw new o.nj("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return i}}))}warnOnIncompatibleInputShape(t){if(null!=this.batchInputShape)if(t.length!==this.batchInputShape.length)console.warn(`The rank of the input tensor provided (shape: ${JSON.stringify(t)}) does not match that of the batchInputShape (${JSON.stringify(this.batchInputShape)}) of the layer ${this.name}`);else{let e=!1;this.batchInputShape.forEach(((s,n)=>{null!=s&&null!=t[n]&&t[n]!==s&&(e=!0)})),e&&console.warn(`The shape of the input tensor (${JSON.stringify(t)}) does not match the expectation of layer ${this.name}: ${JSON.stringify(this.batchInputShape)}`)}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new o.j1(`The layer ${this.name} has never been called and thus has no defined output shape.`);const t=[];for(const e of this.inboundNodes){const s=JSON.stringify(e.outputShapes);-1===t.indexOf(s)&&t.push(s)}if(1===t.length){const t=this.inboundNodes[0].outputShapes;return Array.isArray(t)&&Array.isArray(t[0])&&1===t.length?t[0]:t}throw new o.j1(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`)}countParams(){if(!this.built)throw new o.LH(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);return u.t(this.weights)}build(t){this.built=!0}getWeights(t=!1){return(0,c.FQ)(t?this.trainableWeights:this.weights)}setWeights(t){(0,n.lub)((()=>{const e=this.weights;if(e.length!==t.length)throw new o.nu(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${t.length}, but the layer was expecting ${e.length} weights. Provided weights: ${t}...`);if(0===e.length)return;const s=[],i=(0,c.FQ)(e);for(let a=0;a<i.length;++a){const r=i[a],h=e[a],l=t[a];if(!n.D5U.arraysEqual(r.shape,l.shape))throw new o.nu(`Layer weight shape ${r.shape} not compatible with provided weight shape ${l.shape}`);s.push([h,l])}(0,c.zb)(s)}))}addWeight(t,e,s,n,i,a,h,l){if(-1!==this._addedWeightNames.indexOf(t))throw new o.nu(`Duplicate weight name ${t} for layer ${this.name}`);this._addedWeightNames.push(t),null==s&&(s="float32"),this.fastWeightInitDuringBuild&&(n=null!=l?l():(0,r.L5)("zeros"));const u=n.apply(e,s),p=new c.fU(u,s,t,a,h);return u.dispose(),null!=i&&this.addLoss((()=>i.apply(p.read()))),null==a&&(a=!0),a?this._trainableWeights.push(p):this._nonTrainableWeights.push(p),p}setFastWeightInitDuringBuild(t){this.fastWeightInitDuringBuild=t}addLoss(t){null==t||Array.isArray(t)&&0===t.length||(t=h.zZ(t),void 0!==this._losses&&null!==this._losses&&this.losses.push(...t))}computeOutputShape(t){return t}computeMask(t,e){if(!this.supportsMasking){if(null!=e){if(!Array.isArray(e))throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);e.forEach((t=>{if(null!=t)throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`)}))}return null}return e}addInboundNode(t,e,s,n,i,a,o=null){const r=h.zZ(t);e=h.zZ(e),s=h.zZ(s),n=h.zZ(n),i=l.x6(i),a=l.x6(a);const u=[],c=[],p=[];for(const t of r)u.push(t.sourceLayer),c.push(t.nodeIndex),p.push(t.tensorIndex);new g({outboundLayer:this,inboundLayers:u,nodeIndices:c,tensorIndices:p,inputTensors:r,outputTensors:e,inputMasks:s,outputMasks:n,inputShapes:i,outputShapes:a},o);for(let t=0;t<e.length;t++)e[t].sourceLayer=this,e[t].nodeIndex=this.inboundNodes.length-1,e[t].tensorIndex=t}getConfig(){const t={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(t.batchInputShape=this.batchInputShape),null!=this.dtype&&(t.dtype=this.dtype),t}disposeWeights(){return this.weights.forEach((t=>t.dispose())),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw new Error(`Layer '${this.name}' is already disposed.`)}dispose(){if(!this.built)throw new Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);if(null===this._refCount)throw new Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);this.assertNotDisposed();let t=0;return 0==--this._refCount&&(t=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:t}}}function b(t,e,s){if((null==e||null!=s&&s>0)&&(e=t.sourceLayer,s=t.nodeIndex),0===e.inboundNodes.length)return[t];{const t=e.inboundNodes[s];if(0===t.inboundLayers.length)return t.inputTensors;{const e=[];for(let s=0;s<t.inboundLayers.length;s++){const n=b(t.inputTensors[s],t.inboundLayers[s],t.nodeIndices[s]);for(const t of n)-1===e.indexOf(t)&&e.push(t)}return e}}}},5540:(t,e,s)=>{s.d(e,{QV:()=>V});var n=s(88478),i=s(39840),a=s(28891),o=s(48090),r=s(40588),h=s(49897),l=s(73146),u=s(86275),c=s(38678),p=s(92328),d=s(38374),f=s(2931),g=s(30618),m=s(96040),y=s(51977),b=s(77385),w=s(79608),x=s(87538),$=s(41653),N=s(64843),v=s(34396),k=s(20163);class L extends k.mh{constructor(t){if(super({}),this.containerNodes=new Set,this.name=t.name,null==this.name){const t=this.getClassName().toLowerCase();this.name=(0,w.s)(t)}if(this.supportsMasking=!1,this.trainable_=!0,Array.isArray(t.inputs)?this.inputs=t.inputs.slice():this.inputs=[t.inputs],Array.isArray(t.outputs)?this.outputs=t.outputs.slice():this.outputs=[t.outputs],f.Tw(this.inputs).length!==this.inputs.length)throw new r.nu(`The list of inputs passed to the model is redundant. All inputs should only appear once. Found: ${this.inputs.map((t=>t.name))}`);f.Tw(this.outputs).length!==this.outputs.length&&console.warn(`The list of outputs passed to the model is redundant. All outputs should only appear once. Found: ${this.outputs.map((t=>t.name))}`),this.inputLayers=[],this.inputLayersNodeIndices=[],this.inputLayersTensorIndices=[],this.outputLayers=[],this.outputLayersNodeIndices=[],this.outputLayersTensorIndices=[],this.layers=[],this.internalContainerRefs=[];for(const t of this.outputs){const e=t.sourceLayer,s=t.nodeIndex,n=t.tensorIndex;this.outputLayers.push(e),this.outputLayersNodeIndices.push(s),this.outputLayersTensorIndices.push(n)}for(const t of this.inputs){const e=t.sourceLayer,s=t.nodeIndex,n=t.tensorIndex;f.hu(0===s,"input layer has >1 nodes"),f.hu(0===n,"input layer has >1 tensors"),this.inputLayers.push(e),this.inputLayersNodeIndices.push(s),this.inputLayersTensorIndices.push(n)}this.inputNames=[],this.outputNames=[],this.feedInputShapes=[],this.feedInputNames=[],this.feedOutputNames=[];for(let e=0;e<this.inputLayers.length;e++){const s=this.inputLayers[e];if(!(s instanceof v.l))throw new TypeError(`Input layers to a LayersModel must be InputLayer objects. Received inputs: ${t.inputs}. Input ${e} (0-based) originates from layer type ${s.getClassName()}.`);this.inputNames.push(s.name),this.feedInputShapes.push(s.batchInputShape),this.feedInputNames.push(s.name)}for(const t of this.outputLayers)this.outputNames.push(t.name);this.internalInputShapes=this.inputs.map((t=>t.shape)),this.internalOutputShapes=this.outputs.map((t=>t.shape));const e={},s={},n={},i={},a={},o=[],h=(t,e,s,n,i,l)=>{null!=n&&null!=i&&null!=l||(n=t.sourceLayer,i=t.nodeIndex,l=t.tensorIndex);const u=n.inboundNodes[i];if(-1!==s.indexOf(u))throw new r.LH(`The tensor ${t.name} at layer "${n.name}" is part of a cycle.`);if(-1!==e.indexOf(u))return;this.containerNodes.add(L.nodeKey(n,i)),n.id in a||(a[n.id]=Object.keys(a).length),-1===s.indexOf(u)&&s.push(u);const c=u.inboundLayers.length;for(let t=0;t<c;t++){const n=u.inputTensors[t],i=u.inboundLayers[t],a=u.nodeIndices[t],o=u.tensorIndices[t];h(n,e,s,i,a,o)}for(e.push(u);s.indexOf(u)>=0;)s.splice(s.indexOf(u),1);o.push(u)},l=[],u=[];for(const t of this.outputs)h(t,l,u);const c=o.slice().reverse();for(const t of c){s[t.id]=t,t.id in e||(e[t.id]=0);let a=e[t.id];const o=null==n[t.outboundLayer.id]?0:n[t.outboundLayer.id];a=Math.max(a,o),n[t.outboundLayer.id]=a,i[t.outboundLayer.id]=t.outboundLayer,e[t.id]=a;for(let n=0;n<t.inboundLayers.length;n++){const i=t.inboundLayers[n],o=t.nodeIndices[n],r=i.inboundNodes[o],h=null==e[r.id]?0:e[r.id];e[r.id]=Math.max(a+1,h),s[r.id]=r}}const p={};for(const t in e){const n=e[t];n in p||(p[n]=[]),p[n].push(s[t])}const d={};for(const t in n){const e=n[t];e in d||(d[e]=[]),d[e].push(i[t])}let g=Object.keys(d).map((t=>parseInt(t,10))).sort(f.L7);this.layers=[];for(const t of g){const e=d[t];e.sort(((t,e)=>{const s=a[t.id],n=a[e.id];return s<n?-1:s>n?1:0}));for(const t of e)t instanceof L&&this.internalContainerRefs.push(t),this.layers.push(t)}this.layersByDepth=d,g=Object.keys(p).map((t=>parseInt(t,10))).sort(f.L7);const m=this.inputs.slice(),y=[];for(const t of g)for(const e of p[t]){const t=e.outboundLayer;if(null!=t){for(const s of e.inputTensors)if(-1===m.indexOf(s))throw new r.LH(`Graph disconnected: cannot obtain value for tensor ${s} at layer "${t.name}". The following previous layers were accessed without issue: ${y}`);for(const t of e.outputTensors)m.push(t);y.push(t.name)}}this.nodesByDepth=p;const b=this.layers.map((t=>t.name));for(const t of b){const e=b.filter((e=>e===t)).length;if(1!==e)throw new r.LH(`The name "${t}" is used ${e} times in the model. All layer names should be unique. Layer names: `+JSON.stringify(b))}this.outboundNodes=[],this.inboundNodes=[],new k.NB({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:this.inputs.map((t=>null)),outputMasks:this.outputs.map((t=>null)),inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs.map((t=>t.shape))}),this.built=!0,this._refCount=1}assertNotDisposed(){if(0===this._refCount)throw new Error(`Container '${this.name}' is already disposed.`)}dispose(){this.assertNotDisposed();const t={refCountAfterDispose:null,numDisposedVariables:0};if(0==--this._refCount){for(const e of this.layers)t.numDisposedVariables+=e.dispose().numDisposedVariables;for(const e of this.internalContainerRefs)t.numDisposedVariables+=e.dispose().numDisposedVariables}return t.refCountAfterDispose=this._refCount,t}get trainable(){return this.trainable_}set trainable(t){this.layers.forEach((e=>{e._trainableWeights.forEach((e=>e.trainable=t))})),this.trainable_=t}get trainableWeights(){if(this._trainableWeights.length>0)throw new r.nu("Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.");if(!this.trainable)return[];let t=[];for(const e of this.layers)t=t.concat(e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.layers)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.layers)e.push(...t.trainableWeights);return e.concat(t)}return t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}loadWeights(t,e=!0){const s={};let n=0;for(const t of this.layers)for(const e of t.weights){if(null!=s[e.originalName])throw new r.nu(`Duplicate weight name: ${e.originalName}`);s[e.originalName]=e,n++}const i=[];for(const n in t){let a=n;if(null==s[n]){const t=n.split("/");a=t.slice(0,-2).concat([t[t.length-1]]).join("/")}if(null!=s[a])i.push([s[a],t[n]]);else if(e)throw new r.nu(`Provided weight data has no target variable: ${n}`);delete s[a]}if(e){const t=[];for(const e in s)t.push(e);if(t.length>0)throw new r.nu(`${t.length} of ${n} weights are not set: ${t}`)}(0,$.zb)(i)}updatedConfig(){const t=this.getConfig(),e={};return e.className=this.getClassName(),e.config=t,e.kerasVersion=`tfjs-layers ${b.i}`,e.backend="TensorFlow.js",e}toJSON(t,e=!0){const s=(0,y.q)(this.updatedConfig());return e?JSON.stringify(s):s}call(t,e){return(0,n.lub)((()=>{t=f.zZ(t);const s=new N.l2;for(let e=0;e<this.inputs.length;++e)s.add(this.inputs[e],t[e]);return(0,N.ht)(this.outputs,s,e)}))}computeMask(t,e){return(0,n.lub)((()=>{let s;return t=f.zZ(t),s=null==e?f.JE(null,t.length):f.zZ(e),this.runInternalGraph(t,s)[1]}))}computeOutputShape(t){const e=x.x6(t);if(e.length!==this.inputLayers.length)throw new r.nu(`Invalid inputShape argument ${t}: model has ${this.inputLayers.length} tensor inputs.`);const s={};for(let t=0;t<e.length;t++){const n=this.inputLayers[t],i=e[t];s[n.name+"_0_0"]=i}const n=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort(f.L7);if(n.length>1)for(const t of n){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer;if(-1!==this.inputLayers.map((t=>t.id)).indexOf(e.id))continue;const n=[];for(let e=0;e<t.inboundLayers.length;e++){const i=t.inboundLayers[e],a=t.nodeIndices[e],o=t.tensorIndices[e],r=s[`${i.name}_${a}_${o}`];n.push(r)}const i=e.computeOutputShape(f.Bq(n)),a=x.x6(i),o=e.inboundNodes.indexOf(t);for(let t=0;t<a.length;t++)s[`${e.name}_${o}_${t}`]=a[t]}}const i=[],a=[];for(let t=0;t<this.outputLayers.length;t++){const e=this.outputLayers[t],s=this.outputLayersNodeIndices[t],n=this.outputLayersTensorIndices[t],i=`${e.name}_${s}_${n}`;a.push(i)}for(let t=0;t<a.length;t++){const e=a[t];f.hu(e in s),i.push(s[e])}return f.Bq(i)}runInternalGraph(t,e){null==e&&(e=f.JE(null,t.length));const s={};for(let n=0;n<this.inputs.length;++n){const i=this.inputs[n],a=t[n],o=e[n];s[i.id]=[a,o]}const n=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort(f.L7);for(const t of n){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer,n=t.inputTensors,i=t.outputTensors,a=new Array;for(const t of n)t.id in s&&a.push(s[t.id]);if(a.length===n.length){let n,o,h,l,u={};if(null!=t.callArgs&&(u=t.callArgs),1===a.length){const[t,s]=a[0];null==u.mask&&(u.mask=s),h=f.zZ(e.call(t,u)),l=f.zZ(e.computeMask(t,s)),n=[t],o=[s]}else n=a.map((t=>t[0])),o=a.map((t=>t[1])),null==u.mask&&(u.mask=o),h=f.zZ(e.call(n,u)),l=f.zZ(e.computeMask(n,o));if(e.activityRegularizer)throw new r.nj("LayersModel invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.");for(let t=0;t<i.length;++t){const e=i[t],n=h[t],a=l[t];s[e.id]=[n,a]}}}}const i=[],a=[],o=[];for(const t of this.outputs){f.hu(t.id in s,`Could not compute output ${t.name} : ${t.id}`);const[e,n]=s[t.id];o.push(e.shape),i.push(e),a.push(n)}return[i,a,o]}buildNodeConversionMap(t){const e={};let s;for(const t of this.layers){s=t instanceof L?1:0;for(let n=0;n<t.inboundNodes.length;n++){const i=L.nodeKey(t,n);this.containerNodes.has(i)&&(e[i]=s,s+=1)}}return e}getLayer(t,e){if(null!=e){if(this.layers.length<=e)throw new r.nu(`Was asked to retrieve layer at index ${e}, but model only has ${this.layers.length} layer(s).`);return this.layers[e]}if(null==t)throw new r.nu("Provide either a layer name or layer index");for(const e of this.layers)if(e.name===t)return e;throw new r.nu(`No such layer: ${t}`)}calculateLosses(){return(0,n.lub)((()=>{const t=[];for(const e of this.layers)for(let s=0;s<e.inboundNodes.length;++s){const n=L.nodeKey(e,s);this.containerNodes.has(n)&&t.push(...e.calculateLosses())}return t}))}getConfig(){const t={name:this.name},e=this.buildNodeConversionMap(this.layers),s=[];for(const t of this.layers){const n=t.getClassName(),i=t.getConfig(),a=[];for(let s=0;s<t.inboundNodes.length;s++){const n=t.inboundNodes[s],i=L.nodeKey(t,s);let o={};if(this.containerNodes.has(i)){if(n.callArgs)try{JSON.stringify(n.callArgs),o=n.callArgs}catch(e){console.warn(`Layer ${t.name} was passed non-serializable keyword arguments: ${n.callArgs}. They will not be included in the serialized model (and thus will be missing at deserialization time).`),o={}}if(n.inboundLayers.length>0){const t=[];for(let s=0;s<n.inboundLayers.length;s++){const i=n.inboundLayers[s],a=n.nodeIndices[s],r=n.tensorIndices[s];let h=e[L.nodeKey(i,a)];null==h&&(h=0),t.push([i.name,h,r,o])}a.push(t)}}}const o={};o.name=t.name,o.className=n,o.config=i,o.inboundNodes=a,s.push(o)}t.layers=s;const n=[];for(let t=0;t<this.inputLayers.length;t++){const s=this.inputLayers[t],i=this.inputLayersNodeIndices[t],a=L.nodeKey(s,i);if(!this.containerNodes.has(a))continue;let o=e[a];null==o&&(o=0);const r=this.inputLayersTensorIndices[t];n.push([s.name,o,r])}t.inputLayers=n;const i=[];for(let t=0;t<this.outputLayers.length;t++){const s=this.outputLayers[t],n=this.outputLayersNodeIndices[t],a=L.nodeKey(s,n);if(!this.containerNodes.has(a))continue;let o=e[a];null==o&&(o=0);const r=this.outputLayersTensorIndices[t];i.push([s.name,o,r])}return t.outputLayers=i,t}static fromConfig(t,e,s={},n=!1){const i={},a={};function o(t,e){t.name in a?a[t.name].push(e):a[t.name]=[e]}function l(t,e){const s=[];let n;for(const a of e){const r=a[0],h=a[1],l=a[2];if(n=null==a[3]?{}:a[3],!(r in i))return void o(t,e);const u=i[r];if(u.inboundNodes.length<=h)return void o(t,e);const c=u.inboundNodes[h];s.push(c.outputTensors[l])}s.length>0&&t.apply(f.Bq(s),n)}function u(t){const s=t.name,a=(0,h.v)(t,null!=e.customObjects?e.customObjects:{});a.setFastWeightInitDuringBuild(n),i[s]=a,t.inboundNodes.forEach((t=>{if(!(t instanceof Array))throw new r.nu(`Corrupted configuration, expected array for nodeData: ${t}`);o(a,t)}))}const c=e.name,p=e.layers;for(const t of p)u(t);for(;!f.nK(a);)for(const t of p){const e=i[t.name];if(e.name in a){const t=a[e.name];delete a[e.name];for(const s of t)l(e,s)}}const d=[],g=[],m=e.inputLayers;for(const t of m){const e=t[0],s=t[1],n=t[2];f.hu(e in i);const a=i[e].inboundNodes[s].outputTensors;d.push(a[n])}const y=e.outputLayers;for(const t of y){const e=t[0],s=t[1],n=t[2];f.hu(e in i);const a=i[e].inboundNodes[s].outputTensors;g.push(a[n])}return new t({inputs:d,outputs:g,name:c})}get stateful(){if(this._stateful)throw new r.nu("Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.");for(const t of this.layers)if(t.stateful)return!0;return!1}resetStates(){(0,n.lub)((()=>{this.layers.forEach((t=>{t.stateful&&t.resetStates()}))}))}}function T(t,e){return function(t,e,s){const n=e.length;if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>null));if(1===n)return Array.isArray(t)&&1===t.length?t:"object"==typeof t&&e[0]in t?[t[e[0]]]:[t];if(Array.isArray(t)){if(t.length!==n)throw new Error(`Provided ${s} is an array of ${t.length} element(s), but the model has ${n} outputs. Make sure a set of weights is provided for each model output.`);return t}if("object"==typeof t&&Object.keys(t).length>0&&"object"==typeof t[Object.keys(t)[0]]){const s=[];return e.forEach((e=>{e in t?s.push(t[e]):s.push(null)})),s}throw new Error(`The model has multiple (${n}) outputs, so ${s} must be either an array with ${n} elements or an object with ${e} keys. Provided ${s} not understood: ${JSON.stringify(t)}`)}(t,e,"classWeight")}async function I(t,e,s,i){if(null!=e||null!=i)throw new Error("Support sampleWeight is not implemented yet");if(null!=s){const e=(0,n.lub)((()=>{if(1===t.shape.length)return(0,n.d9v)(t);if(2===t.shape.length){if(t.shape[1]>1){const e=1;return(0,n.NqF)(t,e)}if(1===t.shape[1])return(0,n.XLQ)(t,[t.shape[0]]);throw new Error(`Encountered unexpected last-dimension size (${t.shape[1]}) during handling of class weights. The size is expected to be >= 1.`)}throw new Error(`Unexpected rank of target (y) tensor (${t.rank}) during handling of class weights. The rank is expected to be 1 or 2.`)})),i=Array.from(await e.data());(0,n.B90)(e);const a=[];return i.forEach((t=>{if(null==s[t])throw new Error(`classWeight must contain all classes in the training data. The class ${t} exists in the data but not in classWeight`);a.push(s[t])})),(0,n.RRF)(a,"float32")}return null}function S(t,e){return(0,n.dC7)(t,e)}function E(t,e){let s,i;const a=e;s=a.xs,i=a.ys,n.D5U.assert(null!=s&&null!=i,(()=>`A Dataset iterator for fitDataset() is expected to generate objects of the form \`{xs: xVal, ys: yVal}\`, where the two values may be \`tf.Tensor\`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates ${e}`));const o=A("input",t.inputNames,s),r=A("output",t.outputNames,i),h=o[0].shape[0];n.D5U.assert(o.length===t.inputs.length,(()=>`LayersModel has ${t.inputs.length} inputs, but the dataset provides ${o.length} inputs.  (Expected input keys: ${JSON.stringify(t.inputNames)})`)),n.D5U.assert(r.length===t.outputs.length,(()=>`LayersModel has ${t.outputs.length} outputs, but the dataset provides ${r.length} outputs.  (Expected output keys: ${JSON.stringify(t.outputNames)})`));for(let e=0;e<o.length;e++)n.D5U.assert(o[e].shape[0]===h,(()=>`Batch size mismatch: input ${t.inputNames[e]} has ${o[e].shape[0]}; expected  ${h} based on input ${t.inputNames[0]}.`));for(let e=0;e<r.length;e++)n.D5U.assert(r[e].shape[0]===h,(()=>`Batch size mismatch: output ${t.outputNames[e]} has ${r[e].shape[0]}; expected  ${h} based on input ${t.inputNames[0]}.`));return{xs:o,ys:r}}function A(t,e,s){if(s instanceof n.esB)return[s];if(Array.isArray(s))return n.D5U.assert(s.length===e.length,(()=>`Received an array of ${s.length} Tensors, but expected ${e.length} to match the ${t} keys ${e}.`)),s;{const n=[];for(const i of e){if(null==s[i])throw new r.nu(`The feature data generated by the dataset lacks the required ${t} key '${i}'.`);n.push(s[i])}return n}}function C(t){return"function"==typeof t.iterator}function D(t){n.D5U.assert(t>0&&Number.isInteger(t),(()=>`batchSize is required to be a positive integer, but got ${t}`))}function z(t,e,s){return null==t?[null]:Array.isArray(t)?t.map((t=>(0,i.c9)(t,e,s-e))):(0,i.c9)(t,e,s-e)}function _(t,e){return n.lub((()=>null==t?null:Array.isArray(t)?t.map((t=>_(t,e))):(0,i.Iq)(t,"int32"===e.dtype?e:n.pju(e,"int32"))))}function O(t,e){const s=[];let n=0,i=null;for(;n<t;)i=n+e,i>=t&&(i=t),s.push([n,i]),n=i;return s}function B(t){const e=[];t instanceof n.esB&&(t=[t]);for(let s=0;s<t.length;++s){const n=t[s];if(1===n.rank)e.push((0,i.dt)(n,1));else{if(0===n.rank)throw new Error("Expected tensor to be at least 1D, but received a 0D tensor (scalar).");e.push(n)}}return e}function M(t,e){if(null==t)return;const s=[];if(e instanceof n.esB)s.push(e.id);else if(Array.isArray(e))e.forEach((t=>s.push(t.id)));else if(null!=e)for(const t in e){const n=e[t];s.push(n.id)}const i=[];if(t instanceof n.esB)-1===s.indexOf(t.id)&&i.push(t);else if(Array.isArray(t))t.forEach((t=>{-1===s.indexOf(t.id)&&i.push(t)}));else if(null!=t)for(const e in t){const n=t[e];-1===s.indexOf(n.id)&&i.push(n)}i.forEach((t=>{t.isDisposed||t.dispose()}))}function W(t){return Array.isArray(t)}function F(t){return!function(t){return t instanceof n.esB}(t)&&!W(t)}function U(t,e,s,n=!0,i=""){if(null==e||0===e.length){if(null!=t){let e=!1;if(W(t)&&t.length>0)e=!0;else if(F(t)){for(const s in t)if(t.hasOwnProperty(s)){e=!0;break}}else e=!0;if(e)throw new r.nu(`Error when checking model ${i} expected no data, but got ${t}`)}return[]}if(null==t)return e.map((t=>null));let a;if(F(t)){a=[];for(const s of e){if(null==t[s])throw new r.nu(`No data provided for "${s}". Need data for each key in: ${e}`);a.push(t[s])}}else if(W(t)){if(t.length!==e.length)throw new r.nu(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see ${e.length} Tensor(s), but instead got the following list of Tensor(s): ${t}`);a=t}else{if(e.length>1)throw new r.nu(`The model ${i} expects ${e.length} Tensor(s), but only received one Tensor. Found: Tensor with shape ${t.shape}`);a=[t]}if(a=B(a),null!=s)for(let t=0;t<e.length;++t){if(null==s[t])continue;const o=a[t];if(o.shape.length!==s[t].length)throw new r.nu(`Error when checking ${i}: expected ${e[t]} to have ${s[t].length} dimension(s). but got array with shape ${o.shape}`);for(let e=0;e<s[t].length;++e){if(0===e&&!n)continue;const a=o.shape[e],h=s[t][e];if(null!=h&&h>=0&&a!==h)throw new r.nu(`${i} expected a batch of elements where each example has shape [${s[t].slice(1,s[t].length)}] (i.e.,tensor shape [*,${s[t].slice(1,s[t].length)}]) but the ${i} received an input with ${o.shape[0]} examples, each with shape [${o.shape.slice(1,o.shape.length)}] (tensor shape [${o.shape}])`)}}return a}function j(t,e,s,n=!0,i=""){let a;if(Array.isArray(t)){if(t.length!==e.length)throw new r.nu(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see ${e.length} Tensor(s), but instead got ${t.length} Tensors(s).`);a=t}else{if(e.length>1)throw new r.nu(`The model expects ${e.length} ${i} Tensors, but only received one Tensor. Found: array with shape ${JSON.stringify(t.shape)}.`);a=[t]}if(null!=s)for(let t=0;t<e.length;++t){if(null==s[t])continue;const o=a[t];if(o.shape.length!==s[t].length)throw new r.nu(`Error when checking ${i}: expected ${e[t]} to have ${s[t].length} dimension(s), but got array with shape ${JSON.stringify(o.shape)}`);for(let a=0;a<s[t].length;++a){if(0===a&&!n)continue;const h=o.shape[a],l=s[t][a];if(null!=l&&l!==h)throw new r.nu(`Error when checking ${i}: expected ${e[t]} to have shape ${JSON.stringify(s[t])} but got array with shape ${JSON.stringify(o.shape)}.`)}}}class V extends L{constructor(t){super(t),this.isTraining=!1}summary(t,e,s=console.log){if(!this.built)throw new r.nu("This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).");(0,g.I)(this,t,e,s)}compile(t){if(null==t.loss&&(t.loss=[]),this.loss=t.loss,"string"==typeof t.optimizer)this.optimizer_=p.j(t.optimizer),this.isOptimizerOwned=!0;else{if(!(t.optimizer instanceof n.gaJ))throw new r.nu("User-defined optimizer must be an instance of tf.Optimizer.");this.optimizer_=t.optimizer,this.isOptimizerOwned=!1}let e=[];if(Array.isArray(t.loss)||"string"==typeof t.loss||"function"==typeof t.loss)if(Array.isArray(t.loss)){if(t.loss.length!==this.outputs.length)throw new r.nu(`When passing an Array as loss, it should have one entry per model output. The model has ${this.outputs.length} output(s), but you passed loss=${t.loss}.`);const s=t.loss;e=s.map((t=>u.U2(t)))}else{const s=u.U2(t.loss);this.outputs.forEach((t=>{e.push(s)}))}else{t.loss=t.loss;for(const e in t.loss)if(-1===this.outputNames.indexOf(e))throw new r.nu(`Unknown entry in loss dictionary: "${e}". Only expected the following keys: ${this.outputNames}`);for(const s of this.outputNames)null==t.loss[s]&&console.warn(`Output "${s}" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ${s} during training`),e.push(u.U2(t.loss[s]))}this.lossFunctions=e,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let t=0;t<this.outputs.length;++t){const e=this.internalOutputShapes[t],s=this.outputNames[t];this.feedOutputNames.push(s),this.feedOutputShapes.push(e),this.feedLossFns.push(this.lossFunctions[t])}const s=[];this.metrics=t.metrics,this.metricsNames=["loss"],this.metricsTensors=[],(0,o.f4)("loss",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==s.indexOf(t))continue;const e=this.lossFunctions[t];this.outputs.length>1&&(this.metricsTensors.push([e,t]),this.metricsNames.push(this.outputNames[t]+"_loss"))}}));const i=function(t,e){if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>[]));let s;if("string"==typeof t||"function"==typeof t)s=[t];else{if(!Array.isArray(t)&&"object"!=typeof t)throw new TypeError(`Type of metrics argument not understood. Expected an string,function, Array, or Object, found: ${t}`);s=t}if(Array.isArray(s))return e.map((t=>s));{const t=[];for(const n of e){let e=s.hasOwnProperty(n)?s[n]:[];Array.isArray(e)||(e=[e]),t.push(e)}return t}}(t.metrics,this.outputNames),a=(t,e,s)=>{this.outputNames.length>1&&(e=this.outputNames[t]+"_"+e),this.metricsNames.push(e),this.metricsTensors.push([s,t])};(0,o.f4)("metric",(()=>{for(let t=0;t<this.outputs.length;++t)-1===s.indexOf(t)&&(e=>{let s,n,i;for(const r of e){if("string"==typeof r&&-1!==["accuracy","acc","crossentropy","ce"].indexOf(r)){const e=this.internalOutputShapes[t];let a;1===e[e.length-1]||this.lossFunctions[t]===u.fO?-1!==["accuracy","acc"].indexOf(r)?n=c._F:-1!==["crossentropy","ce"].indexOf(r)&&(n=c.fO):this.lossFunctions[t]===u.KM?-1!==["accuracy","acc"].indexOf(r)?n=c.TY:-1!==["crossentropy","ce"].indexOf(r)&&(n=c.KM):-1!==["accuracy","acc"].indexOf(r)?n=c.G5:-1!==["crossentropy","ce"].indexOf(r)&&(n=c.uq),-1!==["accuracy","acc"].indexOf(r)?a="acc":-1!==["crossentropy","ce"].indexOf(r)&&(a="ce"),i=n,s=""+a}else{const t=c.U2(r);i=t,s=""+c.aI(r)}let e;(0,o.f4)(s,(()=>{e=i})),a(t,s,e)}})(i[t])})),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&this.trainableWeights.length!==this.collectedTrainableWeights.length&&console.warn("Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?")}evaluate(t,e,s={}){const n=null==s.batchSize?32:s.batchSize;D(n);const i=this.standardizeUserDataXY(t,e,!0,n);try{const a=i[0].concat(i[1]);this.makeTestFunction();const o=this.testFunction,r=this.testLoop(o,a,n,s.verbose,s.steps);return(0,f.Bq)(r)}finally{M(i[0],t),M(i[1],e)}}async evaluateDataset(t,e){return this.makeTestFunction(),async function(t,e,s){const i=null!=(s=s||{}).batches,a=t.testFunction;let o=[];if(s.verbose>0)throw new r.nj("Verbose mode is not implemented yet.");n.D5U.assert(!i||s.batches>0&&Number.isInteger(s.batches),(()=>`Test loop expects \`batches\` to be a positive integer, but received ${JSON.stringify(s.batches)}`));const h="function"==typeof e.next?e:await e.iterator();let l=0,u=0;for(;!i||u<s.batches;){const e=await h.next();if(o=n.lub((()=>{if(e.value){const{xs:s,ys:i}=E(t,e.value),r=s.concat(i),h=n.lub((()=>a(r)));if(n.B90(r),0===u)for(let t=0;t<h.length;++t)o.push((0,n.iD$)(0));const c=r[0].shape[0];for(let t=0;t<h.length;++t){const e=h[t],s=o[t];o[t]=n.lub((()=>n.IHx(o[t],n.dC7(c,e)))),u>0&&n.B90(s)}n.B90(h),l+=c,++u}return o})),e.done){i&&console.warn(`Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least \`batches\` batches (in this case, ${s.batches} batches). You may need to use the repeat() function when building your dataset.`);break}}for(let t=0;t<o.length;++t){const e=o[t];o[t]=n.hiC(o[t],l),n.B90(e)}return(0,f.Bq)(o)}(this,t,e)}checkNumSamples(t,e,s,n="steps"){let i;if(null!=s){if(i=null,null!=e)throw new r.nu(`If ${n} is set, batchSize must be null or undefined.Got batchSize = ${e}`)}else{if(null==t)throw new r.nu(`Either the input data should have a defined shape, or ${n} shoud be specified.`);i=Array.isArray(t)?t[0].shape[0]:t.shape[0]}return i}execute(t,e){if(Array.isArray(e)&&0===e.length)throw new r.nu("`outputs` is an empty Array, which is not allowed.");const s=Array.isArray(e),i=s?e:[e],a=this.retrieveSymbolicTensors(i),o=new N.l2;if(t instanceof n.esB&&(t=[t]),Array.isArray(t)){if(t.length!==this.inputs.length)throw new r.nu(`The number of inputs provided (${t.length}) does not match the number of inputs of this model (${this.inputs.length}).`);for(let e=0;e<this.inputs.length;++e)o.add(this.inputs[e],t[e])}else for(const e of this.inputs){const s=t[e.name];if(null==s)throw new r.nu(`No value is provided for the model's input ${e.name}`);o.add(e,s)}const h=(0,N.ht)(a,o);return s?h:h[0]}retrieveSymbolicTensors(t){const e=(0,f.JE)(null,t.length);let s=t.length;for(const n of this.layers){const i=Array.isArray(n.output)?n.output:[n.output],a=i.map((t=>t.name));for(let n=0;n<t.length;++n){const o=a.indexOf(t[n]);if(-1!==o&&(e[n]=i[o],s--),0===s)break}if(0===s)break}if(s>0){const s=[];throw e.forEach(((e,n)=>{null==e&&s.push(t[n])})),new r.nu(`Cannot find SymbolicTensors for output name(s): ${JSON.stringify(s)}`)}return e}predictLoop(t,e=32,s=!1){return n.lub((()=>{const i=this.checkNumSamples(t);if(s)throw new r.nj("Verbose predictLoop() is not implemented yet.");const a=O(i,e),o=this.outputs.map((t=>[]));for(let e=0;e<a.length;++e)n.lub((()=>{const s=a[e][0],n=a[e][1],i=z(t,s,n),o=[];if(Array.isArray(i))for(let t=0;t<i.length;++t)o.push({key:this.inputs[t],value:i[t]});else o.push({key:this.inputs[0],value:i});const r=new N.l2(o);return(0,N.ht)(this.outputs,r)})).forEach(((t,e)=>o[e].push(t)));return(0,f.Bq)(o.map((t=>n.zoF(t,0))))}))}predict(t,e={}){const s=B(t);j(s,this.inputNames,this.feedInputShapes,!1);try{const n=null==e.batchSize?32:e.batchSize;return D(n),this.predictLoop(s,n)}finally{M(s,t)}}predictOnBatch(t){j(t,this.inputNames,this.feedInputShapes,!0);const e=(Array.isArray(t)?t[0]:t).shape[0];return this.predictLoop(t,e)}standardizeUserDataXY(t,e,s=!0,i){if(null==this.optimizer_)throw new r.LH("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");const a=[];for(let t=0;t<this.feedOutputShapes.length;++t){const e=this.feedOutputShapes[t];this.feedLossFns[t]===u.KM?a.push(e.slice(0,e.length-1).concat([1])):a.push(e)}if(function(t,e,s){const i=(0,f.Tw)(t.map((t=>t.shape[0])));i.sort();const a=(0,f.Tw)(e.map((t=>t.shape[0])));if(a.sort(),i.length>1)throw new r.nu(`All input Tensors (x) should have the same number of samples. Got array shapes: ${JSON.stringify(t.map((t=>t.shape)))}`);if(a.length>1)throw new r.nu(`All target Tensors (y) should have the same number of samples. Got array shapes: ${JSON.stringify(e.map((t=>t.shape)))}`);if(i.length>0&&a.length>0&&!n.D5U.arraysEqual(i,a))throw new r.nu(`Input Tensors should have the same number of samples as target Tensors. Found ${i[0]} input sample(s) and ${a[0]} target sample(s).`)}(t=U(t,this.feedInputNames,this.feedInputShapes,!1,"input"),e=U(e,this.feedOutputNames,a,!1,"target")),function(t,e,s){const n=[u.FD,u.fO,u.uq];for(let i=0;i<t.length;++i){const a=t[i],o=e[i],h=s[i];if(null!=o){if(o===u.uq&&1===a.shape[a.shape.length-1])throw new r.nu(`You are passing a target array of shape ${a.shape} while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].`);if(-1!==n.indexOf(o)){const t=a.shape.slice(1),e=h.slice(1);for(let s=0;s<t.length;++s){const n=t[s],i=e[s];if(null!=i&&n!==i)throw new r.nu(`A target Tensor with shape ${a.shape} was passed for an output of shape ${h}, while using a loss function that expects targets to have the same shape as the output.`)}}}}}(e,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=i&&i>0&&t[0].shape[0]%i!=0)throw new r.nu(`In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size ${i}. Found: ${t[0].shape[0]} sample(s).`);return[t,e]}async standardizeUserData(t,e,s,n,i=!0,a){const[o,r]=this.standardizeUserDataXY(t,e,i,a);if(null!=s)throw new Error("sample weight is not supported yet.");let h=null;if(null!=n){const t=T(n,this.outputNames);h=[];for(let e=0;e<t.length;++e)h.push(await I(r[e],null,t[e]))}return[o,r,h]}testLoop(t,e,s,a=0,o){return n.lub((()=>{const h=this.checkNumSamples(e,s,o,"steps"),l=[];if(a>0)throw new r.nj("Verbose mode is not implemented yet.");if(null!=o)throw new r.nj("steps mode in testLoop() is not implemented yet");{const a=O(h,s),o=(0,n.RRF)((0,m.w6)(0,h));for(let s=0;s<a.length;++s){const r=a[s][0],h=a[s][1],u=i.c9(o,r,h-r),c=_(e,u),p=t(c);if(0===s)for(let t=0;t<p.length;++t)l.push((0,n.iD$)(0));for(let t=0;t<p.length;++t){const e=p[t];l[t]=n.IHx(l[t],n.dC7(h-r,e))}}for(let t=0;t<l.length;++t)l[t]=n.hiC(l[t],h)}return l}))}getDedupedMetricsNames(){const t=this.metricsNames,e=[];for(let s=0;s<t.length;++s){const n=t[s];let i=n;(0,f.QX)(t,n)>1&&(i+=`_${(0,f.QX)(t.slice(0,s),n)}`),e.push(i)}return e}makeTrainFunction(){return t=>{const e=[],s=t.slice(0,this.inputs.length),i=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),a=t.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),o=[],r=this.collectedTrainableWeights.map((t=>t.read()));return[this.optimizer_.minimize((()=>{const t=[];for(let e=0;e<this.inputs.length;++e)t.push({key:this.inputs[e],value:s[e]});const r=new N.l2(t),h=(0,N.ht)(this.outputs,r,{training:!0});let l;for(let t=0;t<this.lossFunctions.length;++t){let s=(0,this.lossFunctions[t])(i[t],h[t]);null!=a[t]&&(s=S(s,a[t]));const o=n.J69(s);e.push(o),l=0===t?s:n.IHx(l,s)}for(let t=0;t<this.metricsTensors.length;++t){let s;if(this.outputs.length>1&&t<this.outputs.length)s=e[t];else{const e=this.metricsTensors[t][0],a=this.metricsTensors[t][1];s=n.J69(e(i[a],h[a]))}n.CnY(s),o.push(s)}return l=n.J69(l),this.calculateLosses().forEach((t=>{l=n.IHx(l,t)})),l}),!0,r)].concat(o)}}makeTestFunction(){this.testFunction=t=>n.lub((()=>{const e=[];let s;const i=t.slice(0,this.inputs.length),a=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),o=[];for(let t=0;t<this.inputs.length;++t)o.push({key:this.inputs[t],value:i[t]});const r=new N.l2(o),h=(0,N.ht)(this.outputs,r);for(let t=0;t<this.lossFunctions.length;++t){const i=this.lossFunctions[t],o=n.J69(i(a[t],h[t]));s=0===t?o:n.IHx(s,o),e.push(s)}for(let t=0;t<this.metricsTensors.length;++t){const s=this.metricsTensors[t][0],i=this.metricsTensors[t][1],o=n.J69(s(a[i],h[i]));e.push(o)}return e}))}async fit(t,e,s={}){if(this.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");let i,o,h,l,u,c,p,d,f;this.isTraining=!0;try{const g=null==s.batchSize?32:s.batchSize;D(g);const m=!1,y=await this.standardizeUserData(t,e,s.sampleWeight,s.classWeight,m,g);i=y[0],o=y[1],f=y[2];let b,w=!1;if(null!=s.validationData&&s.validationData.length>0){if(w=!0,2!==s.validationData.length)throw 3===s.validationData.length?new r.nj("validationData including sample weights is not supported yet."):new r.nu(`When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; ${s.validationData} is invalid.`);u=s.validationData[0],c=s.validationData[1];const t=!0,e=await this.standardizeUserData(u,c,null,null,t,g);p=e[0],d=e[1],b=p.concat(d)}else if(null!=s.validationSplit&&s.validationSplit>0&&s.validationSplit<1){w=!0;const t=Math.floor(i[0].shape[0]*(1-s.validationSplit)),e=i[0].shape[0];p=z(i,t,e),h=i,i=z(i,0,t),d=z(o,t,e),l=o,o=z(o,0,t),b=p.concat(d)}else null!=s.validationSteps&&(w=!0);const x=i.concat(o).concat(f);this.checkTrainableWeightsConsistency();const $=this.makeTrainFunction(),N=this.getDedupedMetricsNames();let v,k;w?(this.makeTestFunction(),v=this.testFunction,k=N.slice().concat(N.map((t=>"val_"+t)))):(v=null,b=[],k=N.slice());const L=(0,a.CZ)(s.callbacks,s.yieldEvery);return await this.fitLoop($,x,N,g,s.epochs,s.verbose,L,v,b,s.shuffle,k,s.initialEpoch,null,null)}finally{this.isTraining=!1,M(i,t),M(o,e),M(h,t),M(l,e),M(p,u),M(d,c),null!=f&&n.B90(f)}}async fitLoop(t,e,s,o,h,u,c,p,d,f,g,y,b,w){null==o&&(o=32),null==h&&(h=1),null==f&&(f=!0),null==y&&(y=0);let x=!1;if(null!=p&&null!=d&&(x=!0),null!=w&&(x=!0,null==b))throw new r.nu("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");const $=this.checkNumSamples(e,o,b,"steps_per_epoch");let N;null!=$&&(N=(0,m.w6)(0,$)),null==u&&(u=1);const{callbackList:v,history:k}=(0,a.m$)(c,u,h,y,$,b,o,x,g);v.setModel(this),this.history=k,await v.onTrainBegin(),this.stopTraining_=!1;for(let a=y;a<h;++a){await v.onEpochBegin(a);const h={};if(null!=b)throw new r.nj("stepsPerEpoch mode is not implemented yet.");{if("batch"===f)throw new r.nj("batch shuffling is not implemneted yet");f&&n.D5U.shuffle(N);const a=(0,n.RRF)(N),u=O($,o);for(let r=0;r<u.length;++r){const c={};if(await v.onBatchBegin(r,c),n.lub((()=>{const l=u[r][0],f=u[r][1],g=i.c9(a,l,f-l);c.batch=r,c.size=f-l;const m=_(e,g),y=t(m);for(let t=0;t<s.length;++t){const e=s[t],i=y[t];c[e]=i,n.CnY(i)}if(r===u.length-1&&x){const t=this.testLoop(p,d,o);for(let e=0;e<s.length;++e){const i=s[e],a=t[e];n.CnY(a),h["val_"+i]=a}}})),await v.onBatchEnd(r,c),(0,l.i)(c),this.stopTraining_)break}a.dispose()}if(await v.onEpochEnd(a,h),this.stopTraining_)break}return await v.onTrainEnd(),await this.history.syncData(),this.history}async fitDataset(t,e){return async function(t,e,s){const i=null!=s.batchesPerEpoch;if(n.D5U.assert(null!=t.optimizer,(()=>"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).")),n.D5U.assert(null!=s,(()=>"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.")),n.D5U.assert(null!=s.epochs&&s.epochs>0&&Number.isInteger(s.epochs),(()=>`For fitDataset(), config.epochs is expected to be a positive integer, but got ${s.epochs}`)),n.D5U.assert(!i||s.batchesPerEpoch>0&&Number.isInteger(s.batchesPerEpoch),(()=>`For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got ${s.batchesPerEpoch}`)),n.D5U.assert(null==s.validationSplit,(()=>"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.")),t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");t.isTraining=!0;try{const o=null!=s.validationData;let h,u;if(o)if(C(s.validationData))n.D5U.assert(null==s.validationBatches||s.validationBatches>0&&Number.isInteger(s.validationBatches),(()=>`For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got ${s.validationBatches}`));else{const t=function(t){if(3===t.length)throw new r.nj("Validation with sample weights is not implemented yet.");return{xs:t[0],ys:t[1]}}(s.validationData);h=t.xs,u=t.ys}const c=t.makeTrainFunction(),p=t.getDedupedMetricsNames();let d;d=o?p.slice().concat(p.map((t=>"val_"+t))):p.slice();const g=(0,a.CZ)(s.callbacks,s.yieldEvery),m=null==s.verbose?1:s.verbose,{callbackList:y,history:b}=(0,a.m$)(g,m,s.epochs,null,null,function(t,e){let s=null;return null!=e.batchesPerEpoch?s=e.batchesPerEpoch:Number.isFinite(t.size)&&(s=t.size),s}(e,s),null,o,d);y.setModel(t),t.history=b,await y.onTrainBegin(),t.stopTraining_=!1;let w=null==s.initialEpoch?0:s.initialEpoch,x=await e.iterator();for(;w<s.epochs;){const a={};await y.onEpochBegin(w);let r=0,d=0;for(i||(x=await e.iterator());!i||r<s.batchesPerEpoch;){const e=await x.next();if(i&&e.done){console.warn(`You provided \`batchesPerEpoch\` as ${s.batchesPerEpoch}, but your dataset iterator ran out of data after ${r} batches; interrupting training. Make sure that your dataset can generate at least \`batchesPerEpoch * epochs\` batches (in this case, `+s.batchesPerEpoch*s.epochs+" batches). You may need to use the repeat() function when building your dataset.");break}if(null!=e.value){const{xs:i,ys:a}=E(t,e.value),o={};o.batch=d,o.size=i[0].shape[0],await y.onBatchBegin(d,o);const h=[];if(null!=s.classWeight){const e=T(s.classWeight,t.outputNames);for(let t=0;t<e.length;++t)h.push(await I(a[t],null,e[t]))}const u=i.concat(a).concat(h),f=c(u);n.B90(u);for(let t=0;t<p.length;++t){const e=p[t],s=f[t];o[e]=s,n.CnY(s)}await y.onBatchEnd(d,o),(0,l.i)(o),d++,r++}if(i?r>=s.batchesPerEpoch:e.done){if(o){let e;e=C(s.validationData)?(0,f.zZ)(await t.evaluateDataset(s.validationData,{batches:s.validationBatches})):(0,f.zZ)(t.evaluate(h,u,{batchSize:null==s.validationBatchSize?32:s.validationBatchSize,verbose:0}));for(let s=0;s<t.metricsNames.length;++s)a[`val_${t.metricsNames[s]}`]=e[s]}break}if(t.stopTraining_)break}if(await y.onEpochEnd(w,a),w++,t.stopTraining_)break}return await y.onTrainEnd(),await t.history.syncData(),t.history}finally{t.isTraining=!1}}(this,t,e)}async trainOnBatch(t,e){const s=await this.standardizeUserData(t,e),i=s[0],a=s[1],o=this.makeTrainFunction()(i.concat(a)),r=[];for(const t of o){const e=await t.data();r.push(e[0])}return n.B90(o),M(s[0],t),M(s[1],e),(0,f.Bq)(r)}getNamedWeights(t){const e=[],s=null!=t&&t.trainableOnly,n=s?this.trainableWeights:this.weights,i=this.getWeights(s);for(let t=0;t<n.length;++t)s&&!n[t].trainable||e.push({name:n[t].originalName,tensor:i[t]});return e}set stopTraining(t){this.stopTraining_=t}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(t){this.optimizer_!==t&&(this.optimizer_=t,this.isOptimizerOwned=!1)}dispose(){const t=super.dispose();if(0===t.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const e=n.sq6().numTensors;this.optimizer_.dispose(),t.numDisposedVariables+=e-n.sq6().numTensors}return t}getLossIdentifiers(){let t;if("string"==typeof this.loss)t=(0,f.D1)(this.loss);else if(Array.isArray(this.loss)){for(const t of this.loss)if("string"!=typeof t)throw new Error("Serialization of non-string loss is not supported.");t=this.loss.map((t=>(0,f.D1)(t)))}else{const e=Object.keys(this.loss);t={};const s=this.loss;for(const n of e){if("string"!=typeof s[n])throw new Error("Serialization of non-string loss is not supported.");t[n]=(0,f.D1)(s[n])}}return t}getMetricIdentifiers(){if("string"==typeof this.metrics||"function"==typeof this.metrics)return[(0,f.D1)(c.aI(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map((t=>(0,f.D1)(c.aI(t))));{const t={};for(const e in this.metrics)t[e]=(0,f.D1)(c.aI(this.metrics[e]));return t}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(t){if(null!=t.weighted_metrics)throw new Error("Loading weight_metrics is not supported yet.");if(null!=t.loss_weights)throw new Error("Loading loss_weights is not supported yet.");if(null!=t.sample_weight_mode)throw new Error("Loading sample_weight_mode is not supported yet.");const e=(0,y.a)(t.optimizer_config),s=(0,h.v)(e);let n,i;if("string"==typeof t.loss)n=(0,f.zW)(t.loss);else if(Array.isArray(t.loss))n=t.loss.map((t=>(0,f.zW)(t)));else if(null!=t.loss){n={};for(const e in t.loss)n[e]=(0,f.zW)(t.loss[e])}if(Array.isArray(t.metrics))i=t.metrics.map((t=>(0,f.zW)(t)));else if(null!=t.metrics){i={};for(const e in t.metrics)i[e]=(0,f.zW)(t.metrics[e])}this.compile({loss:n,metrics:i,optimizer:s})}async save(t,e){if("string"==typeof t){const e=n.io.getSaveHandlers(t);if(0===e.length)throw new r.nu(`Cannot find any save handlers for URL '${t}'`);if(e.length>1)throw new r.nu(`Found more than one (${e.length}) save handlers for URL '${t}'`);t=e[0]}if(null==t.save)throw new r.nu("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");const s=await n.io.encodeWeights(this.getNamedWeights(e)),i={modelTopology:this.toJSON(null,!1),format:"layers-model",generatedBy:`TensorFlow.js tfjs-layers v${b.i}`,convertedBy:null};if(null!=e&&e.includeOptimizer&&null!=this.optimizer){i.trainingConfig=this.getTrainingConfig();const t="optimizer",{data:e,specs:a}=await n.io.encodeWeights(await this.optimizer.getWeights(),t);s.specs.push(...a),s.data=n.io.concatenateArrayBuffers([s.data,e])}if(null!=this.userDefinedMetadata){const t=!0;(0,d.WE)(this.userDefinedMetadata,this.name,t),i.userDefinedMetadata=this.userDefinedMetadata}return i.weightData=s.data,i.weightSpecs=s.specs,t.save(i)}setUserDefinedMetadata(t){(0,d.WE)(t,this.name),this.userDefinedMetadata=t}getUserDefinedMetadata(){return this.userDefinedMetadata}}V.className="Model",n.m7h.registerClass(V);class P extends V{}P.className="Functional",n.m7h.registerClass(P)}}]);